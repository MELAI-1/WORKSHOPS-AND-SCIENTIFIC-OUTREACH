{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MELAI-1/WORKSHOPS-AND-SCIENTIFIC-OUTREACH/blob/main/I-X%20AI%20in%20Science-Imperial/Tutorial_Phylodynamics_ModelSelection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTpir9J3OK1d"
      },
      "source": [
        "# **Tutorial for phylodynamics model selection**\n",
        "Based on the method developed in Perez M.F. and Gascuel O.PhyloCNN: Improving tree representation and neural network architecture for deep learning from trees in phylodynamics and diversification studies. https://www.biorxiv.org/content/10.1101/2024.12.13.628187v1\n",
        "\n",
        "## **1. Introduction**\n",
        "This tutorial shows how to train a CNN model that classify phylogentic trees of viruses according to three competing epidemiological (phylodynamics) models.\n",
        "\n",
        "Phylodynamics relies on phylogenetic trees, which are build based on aligned genetic sequences of the pathogen taken from infecte individuals. The trees are calibrated (dated) to reflect transmission times.\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1sQ4hClFJs9xZoSo_ScmtuxYMoa05RSXh\" width=\"600\" height=\"300\">\n",
        "\n",
        "Figure 1. from [Guinat et al., 2021](https://www.sciencedirect.com/science/article/pii/S0169534721001300).\n",
        "\n",
        "We will compare three competing epidemiological (phylodynamics) models - Birth-Death (BD), Birth-Death Exposed Infectious (BDEI) and Birth-Death with Superspreaders (BDSS).\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1FxkO0Qisu6m1_Znc76MMbd6ZVUSPWuAl\" width=\"500\" height=\"300\">\n",
        "\n",
        "The simulated trees were encoded by describing the neighborhood (e.g., length of outgoing branches) and main measurements (e.g., date, number of descendants) of all nodes and leaves of the phylogeny.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1FysAnN2H8C312yQFtAWeFW7OSRAAMdrv\" width=\"750\" height=\"750\">\n",
        "\n",
        "## **2. Libraries and Data Loading**\n",
        "We import the required python libraries and then we load phylogenetic trees simulated under each of the 3 models (BD, BDEI, BDSS).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#First you need to download the data.\n",
        "!gdown --id 1GHLYw3EezrtrMkJDBXY8FNZ4FjyV3Vnn"
      ],
      "metadata": {
        "id": "Dru4AD6NQf51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfe1c69a-be72-425d-df90-2ce1406491a1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1GHLYw3EezrtrMkJDBXY8FNZ4FjyV3Vnn\n",
            "From (redirected): https://drive.google.com/uc?id=1GHLYw3EezrtrMkJDBXY8FNZ4FjyV3Vnn&confirm=t&uuid=f17779ba-4cfa-4be3-8390-997cfe194625\n",
            "To: /content/PhyloDyn.zip\n",
            "100% 70.1M/70.1M [00:00<00:00, 128MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Unzip simulations\n",
        "!unzip \"/content/PhyloDyn.zip\""
      ],
      "metadata": {
        "id": "1XUj4uRMTAbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f91c0175-a9b3-49bd-ade1-7840c3897afa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/PhyloDyn.zip\n",
            "   creating: PhyloDyn/\n",
            "  inflating: __MACOSX/._PhyloDyn     \n",
            "  inflating: PhyloDyn/.DS_Store 2    \n",
            "  inflating: __MACOSX/PhyloDyn/._.DS_Store 2  \n",
            "  inflating: PhyloDyn/.DS_Store      \n",
            "  inflating: __MACOSX/PhyloDyn/._.DS_Store  \n",
            "  inflating: PhyloDyn/Encoded_Zurich.csv  \n",
            "  inflating: __MACOSX/PhyloDyn/._Encoded_Zurich.csv  \n",
            "  inflating: PhyloDyn/BDSS_large_100K.csv  \n",
            "  inflating: __MACOSX/PhyloDyn/._BDSS_large_100K.csv  \n",
            "  inflating: PhyloDyn/Encoded_Zurich.npy  \n",
            "  inflating: __MACOSX/PhyloDyn/._Encoded_Zurich.npy  \n",
            "   creating: PhyloDyn/testset/\n",
            "  inflating: __MACOSX/PhyloDyn/._testset  \n",
            "  inflating: PhyloDyn/Encoded_trees_BDSS.npy  \n",
            "  inflating: __MACOSX/PhyloDyn/._Encoded_trees_BDSS.npy  \n",
            "  inflating: PhyloDyn/Encoded_trees_BDEI.npy  \n",
            "  inflating: __MACOSX/PhyloDyn/._Encoded_trees_BDEI.npy  \n",
            "  inflating: PhyloDyn/Encoded_trees_BD.npy  \n",
            "  inflating: __MACOSX/PhyloDyn/._Encoded_trees_BD.npy  \n",
            "  inflating: PhyloDyn/testset/.DS_Store  \n",
            "  inflating: __MACOSX/PhyloDyn/testset/._.DS_Store  \n",
            "  inflating: PhyloDyn/testset/BDSS_large_10000.csv  \n",
            "  inflating: __MACOSX/PhyloDyn/testset/._BDSS_large_10000.csv  \n",
            "  inflating: PhyloDyn/testset/Encoded_trees_BDSS.npy  \n",
            "  inflating: __MACOSX/PhyloDyn/testset/._Encoded_trees_BDSS.npy  \n",
            "  inflating: PhyloDyn/testset/Encoded_trees_BDEI.npy  \n",
            "  inflating: __MACOSX/PhyloDyn/testset/._Encoded_trees_BDEI.npy  \n",
            "  inflating: PhyloDyn/testset/Encoded_trees_BD.npy  \n",
            "  inflating: __MACOSX/PhyloDyn/testset/._Encoded_trees_BD.npy  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "GmQp3Do2OK1f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Activation, Dense\n",
        "from keras.layers import Conv2D, GlobalAveragePooling2D, BatchNormalization\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# 1) Load tree encodings for BD, BDEI, BDSS models. For each model, we will load tree\n",
        "# encodings for training (1,000 trees per model) and for testing (testset - 100 trees per model).\n",
        "# The encodings are separated in two channels (one for internal nodes and another\n",
        "# for the leaves of the tree). The trees have a maximum of 500 tips (leaves).\n",
        "encoding_BD = np.load('/content/PhyloDyn/Encoded_trees_BD.npy')\n",
        "encoding_test_BD = np.load('/content/PhyloDyn/testset/Encoded_trees_BD.npy')\n",
        "encoding_BDEI  = np.load('/content/PhyloDyn/Encoded_trees_BDEI.npy')\n",
        "encoding_test_BDEI = np.load('/content/PhyloDyn/testset/Encoded_trees_BDEI.npy')\n",
        "encoding_BDSS  = np.load('/content/PhyloDyn/Encoded_trees_BDSS.npy')\n",
        "encoding_test_BDSS = np.load('/content/PhyloDyn/testset/Encoded_trees_BDSS.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1: Data Visualization\n",
        "\n",
        "**Question 1a (add code below and copy it on your assessment form at the end):** Fill the code cell below to check the shape of the loaded inputs for training trees and test trees."
      ],
      "metadata": {
        "id": "2v98FriXb5PF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add code below to recover the shape from encodings of training and test trees.\n",
        "print(f\"the shape of the encodings of training tree is\",encoding_BD.shape)\n",
        "print(f\"the shape of the encodings of encoding_test tree is\",encoding_test_BD.shape)\n"
      ],
      "metadata": {
        "id": "uWD4nvudSUT1",
        "outputId": "03733479-6190-42ff-b0ad-3d10eaa1377a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the shape of the encodings of training tree is (1000, 500, 19, 2)\n",
            "the shape of the encodings of encoding_test tree is (100, 500, 19, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1b (add the answer to your assessment form):** What does the second dimension (value 500) in the arrays represent?\n",
        "- A. The number of simulations  \n",
        "- B. The number of features per node  \n",
        "- C. The number of mavimum leaves per tree  \n",
        "- D. The number of epidemiological models  "
      ],
      "metadata": {
        "id": "Z0il1Q-FWz1P"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dV0S2DXOK1g"
      },
      "source": [
        "## 3. Data Preprocessing\n",
        "We will process the input to be properly formatted before feeding it to the neural network. This will involve the following steps:\n",
        "\n",
        "### Label Assignment\n",
        "We create a label array **Y** for the training and test set, with:\n",
        "- `0` for BD\n",
        "- `1` for BDEI\n",
        "- `2` for BDSS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "xjVNkxGHOK1g"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Add labels for each simulation (a different label for each model)\n",
        "Y = [0 for i in range(len(encoding_BD))]\n",
        "Y.extend([1 for i in range(len(encoding_BDEI))])\n",
        "Y.extend([2 for i in range(len(encoding_BDSS))])\n",
        "Y = np.array(Y)\n",
        "\n",
        "Y_test = [0 for i in range(len(encoding_test_BD))]\n",
        "Y_test.extend([1 for i in range(len(encoding_test_BDEI))])\n",
        "Y_test.extend([2 for i in range(len(encoding_test_BDSS))])\n",
        "Y_test = np.array(Y_test)\n",
        "\n",
        "#We **one-hot encode** `Y` (since it’s a 3-class classification)\n",
        "Y = np.eye(3)[Y]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine encodings from the 3 models\n",
        "encoding = np.concatenate((encoding_BD,encoding_BDEI,encoding_BDSS),axis=0)\n",
        "encoding_test = np.concatenate((encoding_test_BD,encoding_test_BDEI,encoding_test_BDSS),axis=0)"
      ],
      "metadata": {
        "id": "d7jHL3JaZBjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Splitting Data into Training & Validation\n",
        "# 30% for validation\n",
        "Y, Y_valid, encoding, encoding_valid = train_test_split(Y,encoding,test_size=0.3, shuffle=True,stratify=Y)"
      ],
      "metadata": {
        "id": "zQWCTwkTcbJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 2: Data split and stratification\n",
        "\n",
        "**Question 2 (add explanation to the assessment form):** a) What is the validation set and why is it useful? b) Why do we need to shuffle the order of labels and trees? c) What is the advantage to using 'stratify=Y' in our example?"
      ],
      "metadata": {
        "id": "FN7Eq9Z5cfGs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGRschtsOK1i"
      },
      "source": [
        "## 4. Building & Training the CNN (2-Generation Context)\n",
        "\n",
        "### Model Definition\n",
        "We define a CNN that processes input of shape `(500, 19, 2)`:\n",
        "- 500 = number of leaves or nodes\n",
        "- 19 = number of features\n",
        "- 2 = channels (leaves, nodes)\n",
        "\n",
        "This architecture was inspired by the fact that internal nodes and leaves contribute differently to the tree likelihood calculation for multi-type birth-death models (MTBD, which includes BD, BDEI and BDSS; see Equation 8 in [Zhukova et al., 2023](https://academic.oup.com/sysbio/article/72/6/1387/7273092))\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1FvkaeBLF42DuYYgePIj3NhKetzK3Abj6\" width=\"1000\" height=\"500\">\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Fzol42i8u8hvSC6DEMDM3ScsoyeW4TTx\" width=\"500\" height=\"340\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 Build the Neural Network Model <p id=\"build\"> </p>"
      ],
      "metadata": {
        "id": "ci-5jh2_du7W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "X1gsAq8gOK1i"
      },
      "outputs": [],
      "source": [
        "# Creation of the Network Model: model definition\n",
        "def build_model():\n",
        "    # Initialize the Sequential model\n",
        "    model = Sequential()\n",
        "\n",
        "    # First convolutional layer:\n",
        "    # - Filters: 32\n",
        "    # - Kernel size: (1, 19), sliding across the second dimension of the input\n",
        "    # - Input shape: (500, 19, 2) where 500 is the number of tree leaves/nodes, 19 is the feature size, and 2 is the number of channels (leaves and nodes)\n",
        "    # - Activation function: ELU (Exponential Linear Unit)\n",
        "    # - Groups: 2 to apply separate convolutions for the two channels (leaves and nodes)\n",
        "    model.add(Conv2D(filters=32, use_bias=False, kernel_size=(1, 19), input_shape=(500, 19, 2), activation='relu', groups=2))\n",
        "\n",
        "    # Apply batch normalization to stabilize and speed up the training process\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Second convolutional layer:\n",
        "    # - Filters: 32\n",
        "    # - Kernel size: (1, 1) to process each feature independently\n",
        "    # - Activation function: ELU\n",
        "    model.add(Conv2D(filters=32, use_bias=False, kernel_size=(1, 1), activation='relu'))\n",
        "\n",
        "    # Apply batch normalization again\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Third convolutional layer:\n",
        "    # - Filters: 32\n",
        "    # - Kernel size: (1, 1) for further feature processing\n",
        "    # - Activation function: ELU\n",
        "    model.add(Conv2D(filters=32, use_bias=False, kernel_size=(1, 1), activation='relu'))\n",
        "\n",
        "    # Apply batch normalization for the final time before flattening\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Flatten the 2D feature maps from the convolutional layers into a 1D vector,\n",
        "    # which will be passed to the fully connected (dense) layers\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "\n",
        "    # Fully connected (FFNN) part:\n",
        "    # Dense layers with decreasing number of units, all using ELU activation:\n",
        "    model.add(Dense(64, activation='relu'))   # First dense layer with 64 units\n",
        "    model.add(Dense(32, activation='relu'))   # Second dense layer with 32 units\n",
        "    model.add(Dense(16, activation='relu'))   # Third dense layer with 16 units\n",
        "    model.add(Dense(8, activation='relu'))    # Fourth dense layer with 8 units\n",
        "\n",
        "    # Output layer:\n",
        "    # - 3 output neurons, corresponding to the 3 models\n",
        "    # - Activation function: softmax\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    # Show the summary of the model structure (number of layers, shapes of outputs, etc.)\n",
        "    model.summary()\n",
        "\n",
        "    # Return the constructed model\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOohM7-uOK1i"
      },
      "source": [
        "Now we compile and fit the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "EIl2-8hzOK1j"
      },
      "outputs": [],
      "source": [
        "from keras import losses\n",
        "\n",
        "# Initialize the model using the build_model function that was previously defined\n",
        "estimator = build_model()\n",
        "\n",
        "# Compile the model:\n",
        "# - Loss function: categorical_crossentropy is used to measure the error between the predicted probability distribution and the true distribution for multi-class classification tasks.\n",
        "# - Optimizer: 'Adam' is used to minimize the loss function efficiently\n",
        "# - Metrics: Accuracy is used to track the model's performance during training\n",
        "estimator.compile(loss=keras.losses.categorical_crossentropy, optimizer = 'Adam', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping callback to prevent overfitting:\n",
        "# - monitor: monitor the validation accuracy during training\n",
        "# - patience: stop training if the validation accuracy doesn't improve for 100 consecutive epochs\n",
        "# - mode: 'max' indicates that training will stop when the validation accuracy reaches its maximum\n",
        "# - restore_best_weights: restore the weights from the best epoch (the one with the highest validation accuracy)\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=100, mode='max', restore_best_weights=True)\n",
        "\n",
        "# Custom callback to display training progress:\n",
        "# - Print a dot for every epoch (or newline every 100 epochs) to indicate progress in training\n",
        "class PrintD(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        if epoch % 100 == 0:  # Print a newline every 100 epochs\n",
        "            print('')\n",
        "        print('.', end='')  # Print a dot to indicate progress during each epoch\n",
        "\n",
        "# Set the maximum number of epochs (iterations over the entire dataset)\n",
        "EPOCHS = 1000\n",
        "\n",
        "# Train the model using the `fit` method:\n",
        "# - encoding_pad: The padded training data (inputs)\n",
        "# - Y: The target values (outputs)\n",
        "# - verbose: set to 1 to print progress during training\n",
        "# - epochs: The number of times to iterate over the entire dataset\n",
        "# - validation_split: the fraction of data to use for validation (used to monitor validation loss)\n",
        "# - batch_size: the number of samples per gradient update\n",
        "# - callbacks: list of callbacks to be used during training (early stopping and progress display)\n",
        "history = estimator.fit(encoding, Y, verbose=1, epochs=EPOCHS, validation_data=(encoding_valid, Y_valid), batch_size=32, callbacks=[early_stop, PrintD()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox4ZpuqOOK1j"
      },
      "source": [
        "### Evaluate the trained model\n",
        "We evaluate our classifier by using the test set, which was not seen by the network during training. We plot the results as a confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "TSgTvf8TOK1j"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "predicted_test = np.array(estimator.predict(encoding_test))\n",
        "pred_cat = [i.argmax() for i in predicted_test]\n",
        "\n",
        "# Confusion matrix\n",
        "print(confusion_matrix(Y_test, pred_cat))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 3: Confusion matrix\n",
        "\n",
        "**Question 3 (write the answer at the assessment form):** Examine the confusion matrix produced after evaluating the model on the test set. a) What does the confusion matrix reveal about the model’s performance? b) What indications in the matrix would suggest that the model is biased toward one particular class?"
      ],
      "metadata": {
        "id": "aJ4fAITLqhQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can compare the obtained accuracy with other State of the Art approaches. :\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1mamPD_VCI74Y8LzhnHHNyLzfIFqZA8cO\" width=\"300\" height=\"500\">\n",
        "\n",
        "Note that we are using 1,000 trees per model for training, compared to 4 million trees of each model to train the FFNN-SS and CNN-CBLV of [Voznica et al. (2022)](https://www.nature.com/articles/s41467-022-31511-0#Sec29).\n",
        "\n"
      ],
      "metadata": {
        "id": "SjRS0Iv7VDzc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjuGpXvIOK1j"
      },
      "source": [
        "## 5. Predicting empirical (real) data.\n",
        "Our trained network can now be used to predict the most likely epidemiological model on real datasets.\n",
        "We will use the the phylogenetic tree from [Rasmusen et al. (2017)](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005448) with 200 HIV-1 sequences collected as part of the [Swiss Cohort Study (2010)](https://academic.oup.com/ije/article/39/5/1179/799735).\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Fzc9naQ8ACbL9i6_ZDWh1o8GhIvJ1ql4\" width=\"500\" height=\"340\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "lU0UGNzcOK1j"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "encoding_Zurich = np.load('/content/PhyloDyn/Encoded_Zurich.npy')\n",
        "\n",
        "\n",
        "# predict values for the empirical dataset\n",
        "predicted_emp = np.array(estimator.predict(encoding_Zurich))\n",
        "\n",
        "# Print the results\n",
        "print(\"  BD            BDEI          BDSS\")\n",
        "print(predicted_emp)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 4: Analysis of Empirical Data Predictions\n",
        "\n",
        "**Question 4 (write answer at the assessment form):** The trained model predicts epidemiological models for the HIV data (Zurich dataset). a) Which model was selected, and how does this compare to the results reported in the paper (BDSS with superspreaders)? b) If your prediction differs from BDSS, what factors might explain the discrepancy?"
      ],
      "metadata": {
        "id": "bSM5cu8bxkt2"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}