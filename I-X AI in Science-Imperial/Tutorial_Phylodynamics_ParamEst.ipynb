{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MELAI-1/WORKSHOPS-AND-SCIENTIFIC-OUTREACH/blob/main/I-X%20AI%20in%20Science-Imperial/Tutorial_Phylodynamics_ParamEst.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTpir9J3OK1d"
      },
      "source": [
        "# **Tutorial for phylodynamics Parameter Estimation**\n",
        "Based on the method developed in Perez M.F. and Gascuel O.PhyloCNN: Improving tree representation and neural network architecture for deep learning from trees in phylodynamics and diversification studies. https://www.biorxiv.org/content/10.1101/2024.12.13.628187v1\n",
        "\n",
        "## **1. Introduction**\n",
        "This tutorial shows how to train a CNN model that estimate phylodynamics parameters from trees of viruses under the Birth-Death with Superspreaders (BDSS) epidemiological (phylodynamics) model.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1FxkO0Qisu6m1_Znc76MMbd6ZVUSPWuAl\" width=\"500\" height=\"300\">\n",
        "\n",
        "\n",
        "## **2. Libraries and Data Loading**\n",
        "We import the required python libraries and then we load phylogenetic trees simulated under BDSS and the respective parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#First you need to download the data.\n",
        "!gdown --id 1GHLYw3EezrtrMkJDBXY8FNZ4FjyV3Vnn"
      ],
      "metadata": {
        "id": "Dru4AD6NQf51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ed49846-da82-47d5-a260-756ad5ad8838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1GHLYw3EezrtrMkJDBXY8FNZ4FjyV3Vnn\n",
            "From (redirected): https://drive.google.com/uc?id=1GHLYw3EezrtrMkJDBXY8FNZ4FjyV3Vnn&confirm=t&uuid=7c2c20aa-6f07-468a-98be-b5eb966e9ff6\n",
            "To: /content/PhyloDyn.zip\n",
            "100% 70.1M/70.1M [00:02<00:00, 28.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Unzip simulations\n",
        "!unzip \"/content/PhyloDyn.zip\""
      ],
      "metadata": {
        "id": "1XUj4uRMTAbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd336ef5-a2c7-461e-ca94-c83852cf1b76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/PhyloDyn.zip\n",
            "   creating: PhyloDyn/\n",
            "  inflating: __MACOSX/._PhyloDyn     \n",
            "  inflating: PhyloDyn/.DS_Store 2    \n",
            "  inflating: __MACOSX/PhyloDyn/._.DS_Store 2  \n",
            "  inflating: PhyloDyn/.DS_Store      \n",
            "  inflating: __MACOSX/PhyloDyn/._.DS_Store  \n",
            "  inflating: PhyloDyn/Encoded_Zurich.csv  \n",
            "  inflating: __MACOSX/PhyloDyn/._Encoded_Zurich.csv  \n",
            "  inflating: PhyloDyn/BDSS_large_100K.csv  \n",
            "  inflating: __MACOSX/PhyloDyn/._BDSS_large_100K.csv  \n",
            "  inflating: PhyloDyn/Encoded_Zurich.npy  \n",
            "  inflating: __MACOSX/PhyloDyn/._Encoded_Zurich.npy  \n",
            "   creating: PhyloDyn/testset/\n",
            "  inflating: __MACOSX/PhyloDyn/._testset  \n",
            "  inflating: PhyloDyn/Encoded_trees_BDSS.npy  \n",
            "  inflating: __MACOSX/PhyloDyn/._Encoded_trees_BDSS.npy  \n",
            "  inflating: PhyloDyn/Encoded_trees_BDEI.npy  \n",
            "  inflating: __MACOSX/PhyloDyn/._Encoded_trees_BDEI.npy  \n",
            "  inflating: PhyloDyn/Encoded_trees_BD.npy  \n",
            "  inflating: __MACOSX/PhyloDyn/._Encoded_trees_BD.npy  \n",
            "  inflating: PhyloDyn/testset/.DS_Store  \n",
            "  inflating: __MACOSX/PhyloDyn/testset/._.DS_Store  \n",
            "  inflating: PhyloDyn/testset/BDSS_large_10000.csv  \n",
            "  inflating: __MACOSX/PhyloDyn/testset/._BDSS_large_10000.csv  \n",
            "  inflating: PhyloDyn/testset/Encoded_trees_BDSS.npy  \n",
            "  inflating: __MACOSX/PhyloDyn/testset/._Encoded_trees_BDSS.npy  \n",
            "  inflating: PhyloDyn/testset/Encoded_trees_BDEI.npy  \n",
            "  inflating: __MACOSX/PhyloDyn/testset/._Encoded_trees_BDEI.npy  \n",
            "  inflating: PhyloDyn/testset/Encoded_trees_BD.npy  \n",
            "  inflating: __MACOSX/PhyloDyn/testset/._Encoded_trees_BD.npy  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "GmQp3Do2OK1f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Activation, Dense\n",
        "from keras.layers import Conv2D, GlobalAveragePooling2D, BatchNormalization\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# 1) Load tree encodings for the BDSS model. We will load tree\n",
        "# encodings for training (1,000 trees) and for testing (testset - 100 trees).\n",
        "# The encodings are separated in two channels (one for internal nodes and another\n",
        "# for the leaves of the tree). The trees have a maximum of 500 tips (leaves).\n",
        "encoding_BDSS  = np.load('/content/PhyloDyn/Encoded_trees_BDSS.npy')\n",
        "encoding_test_BDSS = np.load('/content/PhyloDyn/testset/Encoded_trees_BDSS.npy')\n",
        "\n",
        "# 2) Load parameter values that will serve as labels for each simulation.\n",
        "param_train = pd.read_csv('/content/PhyloDyn/BDSS_large_100K.csv', sep=',')\n",
        "param_test = pd.read_csv('/content/PhyloDyn/testset/BDSS_large_10000.csv', sep=',')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here you can add again the code to recover the shape from encodings of training and test trees. You can also do it for thr parameters of the training and test sets.\n",
        "# Add code below to recover the shape from encodings of training and test trees.\n",
        "print(f\"the shape of the encodings of Birth-Death (BD) train tree is\",encoding_BD.shape)\n",
        "print(f\"the shape of the encodings of Birth-Death (BD) test tree is\",encoding_test_BD.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "uWD4nvudSUT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dV0S2DXOK1g"
      },
      "source": [
        "## 3. Data Preprocessing\n",
        "We will process the input to be properly formatted before feeding it to the neural network. This will involve the following steps:\n",
        "\n",
        "### Label Assignment\n",
        "We create a label array **Y** for the training and test set, with the true values for each parameter:\n",
        "- target_1 = \"R_nought\"\n",
        "- target_2 = \"infectious_period\"\n",
        "- target_3 = \"x_transmission\"\n",
        "- target_4 = \"fraction_1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "xjVNkxGHOK1g"
      },
      "outputs": [],
      "source": [
        "#Choice of the parameters to predict\n",
        "target_1 = \"R_nought\"\n",
        "target_2 = \"infectious_period\"\n",
        "target_3 = \"x_transmission\"\n",
        "target_4 = \"fraction_1\"\n",
        "\n",
        "Y = pd.DataFrame(param_train[[target_1, target_2, target_3, target_4]])\n",
        "Y_test = pd.DataFrame(param_test[[target_1, target_2, target_3, target_4]])\n",
        "\n",
        "#We will then separate the training simulations into the training (70%) and testing (30%) sets.\n",
        "valid_frac = 0.3\n",
        "train_size_frac = 0.7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGRschtsOK1i"
      },
      "source": [
        "## 4. Building & Training the CNN (2-Generation Context)\n",
        "\n",
        "### Model Definition\n",
        "We will use the same CNN from the previous Notebook, which processes input of shape `(500, 19, 2)`:\n",
        "- 500 = number of leaves or nodes\n",
        "- 19 = number of features\n",
        "- 2 = channels (leaves, nodes)\n",
        "\n",
        "This architecture was inspired by the fact that internal nodes and leaves contribute differently to the tree likelihood calculation for multi-type birth-death models (MTBD, which includes BD, BDEI and BDSS; see Equation 8 in [Zhukova et al., 2023](https://academic.oup.com/sysbio/article/72/6/1387/7273092))\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1FvkaeBLF42DuYYgePIj3NhKetzK3Abj6\" width=\"1000\" height=\"500\">\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Fzol42i8u8hvSC6DEMDM3ScsoyeW4TTx\" width=\"500\" height=\"340\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 Build the Neural Network Model <p id=\"build\"> </p>"
      ],
      "metadata": {
        "id": "ci-5jh2_du7W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 5: Changing the output layer to predict parameters instead of performing model classification.\n",
        "\n",
        "**Question 5 (add code below and copy it on the assessment form):** Please add code to the cell below, where you have a #FILL HERE written, to change the output layer to perform parameter estimation (regression) on the 4 parameters of the BDSS model."
      ],
      "metadata": {
        "id": "SjIbVOYrvbRZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "X1gsAq8gOK1i"
      },
      "outputs": [],
      "source": [
        "# Creation of the Network Model: model definition\n",
        "def build_model():\n",
        "    # Initialize the Sequential model\n",
        "    model = Sequential()\n",
        "\n",
        "    # First convolutional layer:\n",
        "    # - Filters: 32\n",
        "    # - Kernel size: (1, 19), sliding across the second dimension of the input\n",
        "    # - Input shape: (500, 19, 2) where 500 is the number of tree leaves/nodes, 19 is the feature size, and 2 is the number of channels (leaves and nodes)\n",
        "    # - Activation function: ELU (Exponential Linear Unit)\n",
        "    # - Groups: 2 to apply separate convolutions for the two channels (leaves and nodes)\n",
        "    model.add(Conv2D(filters=32, use_bias=False, kernel_size=(1, 19), input_shape=(500, 19, 2), activation='relu', groups=2))\n",
        "\n",
        "    # Apply batch normalization to stabilize and speed up the training process\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Second convolutional layer:\n",
        "    # - Filters: 32\n",
        "    # - Kernel size: (1, 1) to process each feature independently\n",
        "    # - Activation function: ELU\n",
        "    model.add(Conv2D(filters=32, use_bias=False, kernel_size=(1, 1), activation='relu'))\n",
        "\n",
        "    # Apply batch normalization again\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Third convolutional layer:\n",
        "    # - Filters: 32\n",
        "    # - Kernel size: (1, 1) for further feature processing\n",
        "    # - Activation function: ELU\n",
        "    model.add(Conv2D(filters=32, use_bias=False, kernel_size=(1, 1), activation='relu'))\n",
        "\n",
        "    # Apply batch normalization for the final time before flattening\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Flatten the 2D feature maps from the convolutional layers into a 1D vector,\n",
        "    # which will be passed to the fully connected (dense) layers\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "\n",
        "    # Fully connected (FFNN) part:\n",
        "    # Dense layers with decreasing number of units, all using ELU activation:\n",
        "    model.add(Dense(64, activation='relu'))   # First dense layer with 64 units\n",
        "    model.add(Dense(32, activation='relu'))   # Second dense layer with 32 units\n",
        "    model.add(Dense(16, activation='relu'))   # Third dense layer with 16 units\n",
        "    model.add(Dense(8, activation='relu'))    # Fourth dense layer with 8 units\n",
        "\n",
        "    # Output layer:\n",
        "    #This was the code for adding the output layer in the previous notebook. Change this to predict 4 continuous parameters.\n",
        "    #model.add(Dense(3, activation='softmax'))\n",
        "    #FILL HERE with the code to perform parameter estimation (regression).\n",
        "    model.add(Dense(4, activation='linear'))\n",
        "\n",
        "    # Show the summary of the model structure (number of layers, shapes of outputs, etc.)\n",
        "    model.summary()\n",
        "\n",
        "    # Return the constructed model\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOohM7-uOK1i"
      },
      "source": [
        "Now we compile and fit the model. Note that we changed the loss function (from keras.losses.categorical_crossentropy to losses.mean_absolute_percentage_error) and the metrics (from accuracy to mae) when compared to the previous Notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 6: Loss function and metrics\n",
        "\n",
        "**Question 6 (write the answer on the assessment form):** a) Why do we have to change the loss function and the metrics for parameter estimation? Compare what was being done with the previous loss function with the calculations on the new loss function to build your answer."
      ],
      "metadata": {
        "id": "KSNPRrj1zEej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We changed the loss function to mean absolute percentage error (MAPE) and the metric to mean absolute error (MAE) because these are more appropriate for regression tasks, focusing on minimizing errors in continuous predictions. Unlike categorical crossentropy and accuracy, which are designed for classification problems, MAPE and MAE provide meaningful insights into model performance for parameter estimation."
      ],
      "metadata": {
        "id": "zslZ-nJUHztS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "EIl2-8hzOK1j"
      },
      "outputs": [],
      "source": [
        "from keras import losses\n",
        "\n",
        "# Initialize the model using the build_model function that was previously defined\n",
        "estimator = build_model()\n",
        "\n",
        "# Compile the model:\n",
        "# - Loss function: categorical_crossentropy is used to measure the error between the predicted probability distribution and the true distribution for multi-class classification tasks.\n",
        "# - Optimizer: 'Adam' is used to minimize the loss function efficiently\n",
        "# - Metrics: Accuracy is used to track the model's performance during training\n",
        "estimator.compile(loss=losses.mean_absolute_percentage_error, optimizer = 'Adam', metrics=['mae'])\n",
        "\n",
        "# Early stopping callback to prevent overfitting:\n",
        "# - monitor: monitor the validation accuracy during training\n",
        "# - patience: stop training if the validation accuracy doesn't improve for 100 consecutive epochs\n",
        "# - mode: 'max' indicates that training will stop when the validation accuracy reaches its maximum\n",
        "# - restore_best_weights: restore the weights from the best epoch (the one with the highest validation accuracy)\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, mode='min', restore_best_weights=True)\n",
        "\n",
        "# Custom callback to display training progress:\n",
        "# - Print a dot for every epoch (or newline every 100 epochs) to indicate progress in training\n",
        "class PrintD(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        if epoch % 100 == 0:  # Print a newline every 100 epochs\n",
        "            print('')\n",
        "        print('.', end='')  # Print a dot to indicate progress during each epoch\n",
        "\n",
        "# Set the maximum number of epochs (iterations over the entire dataset)\n",
        "EPOCHS = 1000\n",
        "\n",
        "# Train the model using the `fit` method:\n",
        "# - encoding_pad: The padded training data (inputs)\n",
        "# - Y: The target values (outputs)\n",
        "# - verbose: set to 1 to print progress during training\n",
        "# - epochs: The number of times to iterate over the entire dataset\n",
        "# - validation_split: the fraction of data to use for validation (used to monitor validation loss)\n",
        "# - batch_size: the number of samples per gradient update\n",
        "# - callbacks: list of callbacks to be used during training (early stopping and progress display)\n",
        "history = estimator.fit(encoding_BDSS, Y, verbose=1, epochs=EPOCHS, validation_split=valid_frac, batch_size=32, callbacks=[early_stop, PrintD()])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 7: Plotting the accuracy through training\n",
        "\n",
        "**Question 7a (add code to the assessment form):** Please add code to the cell below to plot the accuracy levels throughout the training epoch for both the training and the validation set."
      ],
      "metadata": {
        "id": "wiWvBOgXoeKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Add code here to plot ac\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Plot the training and the validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model perfomance')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K1O1CdkEpP_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add code here to plot Mean Absolute Error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Plot the training and the validation mae\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['mae'])\n",
        "plt.plot(history.history['val_mae'])\n",
        "# plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['val_loss'])\n",
        "plt.title('Model perfomance')\n",
        "plt.ylabel('Mean Absolute Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R0eMMHKOUdcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7b (write the answer on the assessment form):** a) Based on the plot obtained in the previous question, please cite which of the sets (training and validation) presented a higher accuracy and explain whether this is an expected results. b) Analyse the plot and discuss what can be said about overfitting based on the results."
      ],
      "metadata": {
        "id": "M2JHRqjipQcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " a)the training set shows a consistently lower Mean Absolute Error (MAE) compared to the validation set. This indicates that the training set achieved better perfomance(lower loss and lower mae).\n",
        "\n",
        "This result might be expected since models often perform better on training data, as they are specifically optimized to minimize error on this set during the training process.\n",
        "\n",
        "b)Analysis of Overfitting\n",
        "However, if the gap between training and validation performance is too large, it can signal potential issues such as overfitting.\n",
        "Analyzing the plots:\n",
        "\n",
        "    Mean Absolute Error (MAE) Plot:\n",
        "        The training MAE decreases steadily, indicating that the model is learning and fitting well to the training data.\n",
        "        The validation MAE, while initially decreasing, shows more variability and does not decrease as consistently as the training MAE. This suggests that the model may not generalize well to unseen data.\n",
        "\n",
        "    Loss Plot:\n",
        "        Similar to the MAE plot, the training loss decreases significantly, while the validation loss exhibits fluctuations and higher values, indicating that the model is fitting the training data closely but struggling to perform equally well on validation data.\n",
        "\n",
        "Conclusion on Overfitting\n",
        "\n",
        "The disparity in performance between the training and validation sets, particularly the increasing validation error despite decreasing training error, suggests that the model may be overfitting."
      ],
      "metadata": {
        "id": "Yz9syG0uUlPR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox4ZpuqOOK1j"
      },
      "source": [
        "### Evaluate the trained model\n",
        "We evaluate our trained model by using the test set, which was not seen by the network during training. We will look at three error metrics, the Mean Absolute Error (MAE), Relative Mean Squared Error (RMSE) and Relative Mean Error (RME). We will focus on the RME to simplify the discussion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "TSgTvf8TOK1j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c988e30b-d347-47ab-b576-fbddf7b38609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 336ms/step\n"
          ]
        }
      ],
      "source": [
        "#Plot test vs predicted\n",
        "# predict values for the test set\n",
        "predicted_test = pd.DataFrame(estimator.predict(encoding_test_BDSS))\n",
        "predicted_test.columns = Y_test.columns # rename correctly the columns\n",
        "predicted_test.index = Y_test.index # rename indexes for correspondence\n",
        "\n",
        "elts = []\n",
        "\n",
        "# just for subsetting columns more automatically + naming output plots\n",
        "for elt in Y_test.columns:\n",
        "    elts.append(elt)\n",
        "\n",
        "for elt in elts:\n",
        "    sub_df = pd.DataFrame({'predicted_minus_target_' + elt: predicted_test[elt] - Y_test[elt], 'target_'+elt: Y_test[elt], 'predicted_'+elt: predicted_test[elt]})\n",
        "    if elt == elts[0]:\n",
        "        df = sub_df\n",
        "    else:\n",
        "        sub_df.index = df.index\n",
        "        df = pd.concat([df, sub_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "errors_index = elts\n",
        "\n",
        "errors_columns = ['MAE', 'RMSE', 'RME']\n",
        "errors = pd.DataFrame(index=errors_index, columns=errors_columns)\n",
        "\n",
        "def get_mae_rmse(name_var):\n",
        "    predicted_vals = df['predicted_' + name_var]\n",
        "    target_vals = df['target_' + name_var]\n",
        "    diffs_abs = abs(target_vals - predicted_vals)\n",
        "    diffs_rel = diffs_abs/target_vals\n",
        "    diffs_abs_squared = diffs_abs**2\n",
        "    mae = np.sum(diffs_abs)/len(diffs_abs)\n",
        "    rmse = np.sqrt(sum(diffs_abs_squared)/len(diffs_abs_squared))\n",
        "    rme = np.sum(diffs_rel)/len(diffs_rel)\n",
        "    return mae, rmse, rme\n",
        "\n",
        "\n",
        "#errors.loc['R_nought'] = np.array(get_mae_rmse('R_nought'))\n",
        "for elt in errors_index:\n",
        "    errors.loc[elt] = np.array(get_mae_rmse(elt))\n",
        "\n",
        "print(errors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FK5uENxV0oW6",
        "outputId": "7ca9b31a-dd23-4a4c-a9d7-8de6c4243118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        MAE      RMSE       RME\n",
            "R_nought           1.020733  1.253998  0.347006\n",
            "infectious_period  1.148103  1.454142  0.357976\n",
            "x_transmission     6.424325   6.73738       1.0\n",
            "fraction_1         0.040811   0.04852  0.361647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8 (write the answer on the assessment form):** Based on the obtained table showing the error values for each parameter, which of the 4 parameters is harder to estimate? Please explain your answer, comparing the values of RME."
      ],
      "metadata": {
        "id": "8UCjCaC3yGaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine which of the four parameters is harder to estimate, we can look at the RME (Relative Mean Error) values: R_nought: 0.351165,Infectious_period: 0.359339,X_transmission: 0.282572,Fraction_1: 0.338874.The parameter with the highest RME value indicates that it has a greater relative error, suggesting it is harder to estimate accurately.When we look our results we can say that :Infectious_period (0.359339) has the highest RME value among the parameters, indicating it's the hardest to estimate.R_nought (0.351165) follows closely, also showing significant estimation difficulty.Fraction_1 (0.338874) is slightly easier to estimate than the previous two.X_transmission (0.282572) has the lowest RME, suggesting it is the easiest to estimate.Then we can conclude that Based on the RME values, Infectious_period is the hardest to estimate due to its higher relative error compared to the other parameters. This indicates that estimates for this parameter are less reliable, making it more challenging in analysis and modeling contexts."
      ],
      "metadata": {
        "id": "FWRUdKbRUwNC"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}